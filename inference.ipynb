{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/us-army-fm-fine-tuning/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import yaml\n",
    "import torch\n",
    "# from unsloth import FastLanguageModel\n",
    "# from huggingface_hub import login\n",
    "# import argparse\n",
    "# from peft import PeftModel\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", device=device)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"ShethArihant/Llama-3.1-8B-us-army-fm-instruct\",\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "model.eval()\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Are Areas of Operations (AO) always contiguous?\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"No, Areas of Operations (AO) are not always contiguous. Operational areas may be contiguous or noncontiguous. When they are contiguous, a boundary separates them. When operational areas are noncontiguous, subordinate commands do not share a boundary, and the higher headquarters retains responsibility for the unassigned portion of its operational area.\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Are Joint Security Areas (JSAs) always located within areas actively engaged in combat?\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"No, Joint Security Areas (JSAs) are not always located within areas actively engaged in combat. JSAs can be intermixed with combat elements and may include intermediate support bases and other support facilities. They can be used in both linear and nonlinear situations.\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"In an operation with noncontiguous operational areas, are the subordinate units always mutually supporting?\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"No, in an operation with noncontiguous operational areas, the subordinate units are not always mutually supporting. They can be widely distributed and beyond the mutually supporting range of each other. This is true even for a Service or functional component, such as the ground component, which could have such a large operational area that its subordinate units operate in a noncontiguous manner.\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages[:3],\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, Areas of Operations (AO) are not always contiguous. Operational areas may be contiguous or noncontiguous. When they are contiguous, a boundary separates them. When operational areas are noncontiguous, subordinate commands do not share a boundary, and the higher headquarters retains responsibility for the unassigned portion of its operational area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, Joint Security Areas (JSAs) are not always located within areas actively engaged in combat. JSAs are designated areas where the host nation or other security forces have the primary responsibility for security. They can be located in areas that are not actively engaged in combat, such as in rear areas or in areas where stability operations are being conducted. JSAs are used to free up joint forces to focus on combat operations, and they help to build partner capacity and stability in the region.<|eot_id|><|start_header_id|>assistant\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "text_streamer = TextIteratorStreamer(tokenizer)\n",
    "import textwrap\n",
    "max_print_width = 100\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages[:3],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, Joint Security Areas (JSAs) are \n",
      "not always located within areas actively engaged in combat. Joint Security Areas are designated areas \n",
      "established to ensure the security of friendly forces and to facilitate freedom of action. They may be located \n",
      "in areas that are not actively engaged in combat, but are critical to the success of the operation. \n",
      "JSAs can be used to secure key infrastructure, protect critical assets, or provide a safe haven for \n",
      "forces to reposition or reconstitute.\n",
      "\n",
      "JSAs are typically used"
     ]
    }
   ],
   "source": [
    "generation_kwargs = dict(\n",
    "    inputs,\n",
    "    streamer = text_streamer,\n",
    "    max_new_tokens = 100,\n",
    "    use_cache = True,\n",
    ")\n",
    "thread = Thread(target = model.generate, kwargs = generation_kwargs)\n",
    "thread.start()\n",
    "\n",
    "length = 0\n",
    "for j, new_text in enumerate(text_streamer):\n",
    "    if j == 0:\n",
    "        wrapped_text = textwrap.wrap(new_text, width = max_print_width)\n",
    "        length = len(wrapped_text[-1])\n",
    "        wrapped_text = \"\\n\".join(wrapped_text)\n",
    "        # print(wrapped_text, end = \"\")\n",
    "    else:\n",
    "        length += len(new_text)\n",
    "        if length >= max_print_width:\n",
    "            length = 0\n",
    "            print()\n",
    "        print(new_text, end = \"\")\n",
    "    pass\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
